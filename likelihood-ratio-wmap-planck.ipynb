{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to build likelihood ratio estimator\n",
    "\n",
    "Estimating \n",
    "\n",
    "$r = \\frac{P(D_W, D_P, \\theta)}{P(D_W, \\theta) P(D_P)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tensionnet import wmapplanck\n",
    "from cmblike.data import get_data\n",
    "from tqdm import tqdm\n",
    "\n",
    "wmapraw, lwmap = get_data(base_dir='cosmology-data/').get_wmap()\n",
    "praw, l = get_data(base_dir='cosmology-data/').get_planck()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code takes a library of CAMB cls and corresponding parameters and makes a set of observations of the power spectrum from Planck and WMAP as if they were viewing the same sky..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 285/500 [01:36<01:12,  2.97it/s]"
     ]
    }
   ],
   "source": [
    "nSamples = 500\n",
    "joint = wmapplanck.jointClGen()\n",
    "clLibrary = np.load('cl_library/cls.npy')[:nSamples]\n",
    "paramsLibrary = np.load('cl_library/params.npy')[:nSamples]\n",
    "\n",
    "Aplanck = l*(l+1)/(2*np.pi)\n",
    "Awmap = lwmap*(lwmap+1)/(2*np.pi)\n",
    "\n",
    "wmapData = []\n",
    "planckData = []\n",
    "for i in tqdm(range(len(paramsLibrary))):\n",
    "    params = paramsLibrary[i]\n",
    "    cls = clLibrary[i]\n",
    "    planck, wmap = joint([], params, clexample=cls)\n",
    "    wmapData.append(wmap)\n",
    "    planckData.append(planck)\n",
    "wmapData = np.array(wmapData)\n",
    "planckData = np.array(planckData)\n",
    "\n",
    "splitIdx = np.arange(len(wmapData))\n",
    "np.random.shuffle(splitIdx)\n",
    "trainIdx = splitIdx[:int(len(wmapData)*0.8)]\n",
    "testIdx = splitIdx[int(len(wmapData)*0.8):]\n",
    "\n",
    "trainWmap = wmapData[trainIdx]\n",
    "testWmap = wmapData[testIdx]\n",
    "trainPlanck = planckData[trainIdx]\n",
    "testPlanck = planckData[testIdx]\n",
    "trainParams = paramsLibrary[trainIdx]\n",
    "testParams = paramsLibrary[testIdx]\n",
    "\n",
    "normtrainWmapData = (trainWmap -np.mean(trainWmap))/np.std(trainWmap)\n",
    "normtestWmapData = (testWmap -np.mean(trainWmap))/np.std(trainWmap)\n",
    "normtrainPlanckData = (trainPlanck -np.mean(trainPlanck))/np.std(trainPlanck)\n",
    "normtestPlanckData = (testPlanck -np.mean(trainPlanck))/np.std(trainPlanck)\n",
    "normtrainParams = (trainParams -np.mean(trainParams, axis=0))/np.std(trainParams, axis=0)\n",
    "normtestParams = (testParams -np.mean(trainParams, axis=0))/np.std(trainParams, axis=0)\n",
    "\n",
    "\n",
    "matchedtrainData = np.hstack([normtrainWmapData, normtrainParams, normtrainPlanckData])\n",
    "matchedtrainLabels = np.ones(len(matchedtrainData))\n",
    "matchedtestData = np.hstack([normtestWmapData, normtestParams, normtestPlanckData])\n",
    "matchedtestLabels = np.ones(len(matchedtestData))\n",
    "\n",
    "idx = np.arange(len(matchedtrainData))\n",
    "np.random.shuffle(idx)\n",
    "shuffledtrainPlanck = normtrainPlanckData[idx]\n",
    "shuffledtrainData = np.hstack([normtrainWmapData, normtrainParams, shuffledtrainPlanck])\n",
    "shuffledtrainLabels = np.zeros(len(shuffledtrainData))\n",
    "\n",
    "data_train = np.vstack([matchedtrainData, shuffledtrainData])\n",
    "labels_train = np.hstack([matchedtrainLabels, shuffledtrainLabels])\n",
    "\n",
    "idx = np.arange(len(matchedtestData))\n",
    "np.random.shuffle(idx)\n",
    "shuffledtestPlanck = normtestPlanckData[idx]\n",
    "shuffledtestData = np.hstack([normtestWmapData, normtestParams, shuffledtestPlanck])\n",
    "shuffledtestLabels = np.zeros(len(shuffledtestData))\n",
    "\n",
    "data_test = np.vstack([matchedtestData, shuffledtestData])\n",
    "labels_test = np.hstack([matchedtestLabels, shuffledtestLabels])\n",
    "\n",
    "idx = np.arange(len(data_train))\n",
    "np.random.shuffle(idx)\n",
    "data_train = data_train[idx]\n",
    "labels_train = labels_train[idx]\n",
    "\n",
    "idx = np.arange(len(data_test))\n",
    "np.random.shuffle(idx)\n",
    "data_test = data_test[idx]\n",
    "labels_test = labels_test[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "                learning_rate=1e-3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(data_train.shape[1], activation='sigmoid'),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(100, activation='sigmoid',\n",
    "                        kernel_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(100, activation='sigmoid',\n",
    "                        kernel_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(1, activation='linear',\n",
    "                        kernel_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def _test_step(param, truth):\n",
    "        \n",
    "    r\"\"\"\n",
    "    This function is used to calculate the loss value at each epoch and\n",
    "    adjust the weights and biases of the neural networks via the\n",
    "    optimizer algorithm.\n",
    "    \"\"\"\n",
    "    prediction = tf.transpose(model(param, training=True))[0]\n",
    "    prediction = tf.keras.layers.Activation('sigmoid')(prediction)\n",
    "    truth = tf.convert_to_tensor(truth)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(truth, prediction)\n",
    "    return loss\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def _train_step(params, truth):\n",
    "\n",
    "    r\"\"\"\n",
    "    This function is used to calculate the loss value at each epoch and\n",
    "    adjust the weights and biases of the neural networks via the\n",
    "    optimizer algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = tf.transpose(model(params, training=True))[0]\n",
    "        prediction = tf.keras.layers.Activation('sigmoid')(prediction)\n",
    "        truth = tf.convert_to_tensor(truth)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(truth, prediction)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(\n",
    "        zip(gradients,\n",
    "            model.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m      3\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      5\u001b[0m data_train, data_test, labels_train, labels_test \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m----> 6\u001b[0m         train_test_split(\u001b[43mdata\u001b[49m, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m      8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([data_train, labels_train[:, np\u001b[38;5;241m.\u001b[39mnewaxis]])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(train_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 50\n",
    "batch_size = 300\n",
    "patience = 10\n",
    "\n",
    "train_dataset = np.hstack([data_train, labels_train[:, np.newaxis]]).astype(np.float32)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = np.hstack([data_test, labels_test[:, np.newaxis]]).astype(np.float32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_dataset)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "loss_history = []\n",
    "test_loss_history = []\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "c = 0\n",
    "#for i in tqdm.tqdm(range(epochs)):\n",
    "for i in range(epochs):\n",
    "\n",
    "    loss = [_train_step(x[:, :-1], x[:, -1]) for x in  train_dataset]\n",
    "    epoch_loss_avg.update_state(loss)\n",
    "    loss_history.append(epoch_loss_avg.result())\n",
    "\n",
    "    test_loss = [_test_step(x[:, :-1], x[:, -1]) for x in  test_dataset]\n",
    "    test_loss_history.append(tf.reduce_mean(test_loss))\n",
    "    print('Epoch: {} Loss: {:.5f} Test Loss: {:.5f}'.format(\n",
    "        i, loss_history[-1], test_loss_history[-1]))\n",
    "\n",
    "    c += 1\n",
    "    if i == 0:\n",
    "        minimum_loss = test_loss_history[-1]\n",
    "        minimum_epoch = i\n",
    "        minimum_model = model\n",
    "    else:\n",
    "        if test_loss_history[-1] < minimum_loss:\n",
    "            minimum_loss = test_loss_history[-1]\n",
    "            minimum_epoch = i\n",
    "            minimum_model = model\n",
    "            c = 0\n",
    "    if minimum_model:\n",
    "        if c == patience:\n",
    "            print('Early stopped. Epochs used = ' + str(i) +\n",
    "                    '. Minimum at epoch = ' + str(minimum_epoch))\n",
    "            model = minimum_model\n",
    "            break\n",
    "\n",
    "plt.plot(loss_history, label='train')\n",
    "plt.plot(test_loss_history, label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_history.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (45,) (83,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m wmap_noise \u001b[38;5;241m=\u001b[39m wmap_noise(l)\u001b[38;5;241m.\u001b[39mcalculate_noise()\n\u001b[1;32m      5\u001b[0m cmbs \u001b[38;5;241m=\u001b[39m CMB()\n\u001b[0;32m----> 6\u001b[0m likelihood \u001b[38;5;241m=\u001b[39m \u001b[43mcmbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwmap_noise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(planck, wmap, params, nonNormParams):\n\u001b[1;32m     10\u001b[0m         r_values \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/Current-projects/tension-networks/myenv/lib/python3.11/site-packages/cmblike/cmb.py:97\u001b[0m, in \u001b[0;36mCMB.get_likelihood\u001b[0;34m(self, data, l, noise)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# if noise power add to the data \u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlikelihood\u001b[39m(theta):\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    The likelihood function. Generates a realisation of the\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    CMB power spectrum from the CAMB model and compares it\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m        The log likelihood.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (45,) (83,) "
     ]
    }
   ],
   "source": [
    "from cmblike.noise import wmap_noise\n",
    "from cmblike.cmb import CMB\n",
    "\n",
    "wmap_noise = wmap_noise(l).calculate_noise()\n",
    "cmbs = CMB()\n",
    "likelihood = cmbs.get_likelihood(wmap, lwmap, noise=wmap_noise)\n",
    "\n",
    "def __call__(planck, wmap, params, nonNormParams):\n",
    "\n",
    "        r_values = []\n",
    "        for i in range(len(params)):\n",
    "            ps = tf.convert_to_tensor(np.array([[*wmap, *params[i], *planck]]).astype('float32'))\n",
    "            logr = model(ps).numpy()[0]\n",
    "            r_values.append(logr + likelihood(nonNormParams[i])[0] + 587.5)\n",
    "\n",
    "        r_values = np.array(r_values).T[0]\n",
    "        return r_values\n",
    "\n",
    "\n",
    "normWmap = (wmap - np.mean(trainWmap))/np.std(trainWmap)\n",
    "normPlanck = (praw - np.mean(trainPlanck))/np.std(trainPlanck)\n",
    "\n",
    "samples = paramsLibrary[:100]\n",
    "norm_params = (samples - np.mean(trainParams, axis=0))/np.std(trainParams, axis=0)\n",
    "\n",
    "# logL - log Z\n",
    "r_values = __call__(normPlanck, normWmap, norm_params, samples)\n",
    "\n",
    "cbar = plt.scatter(samples[:, 0], samples[:, 1], c=r_values, s=10, cmap='viridis')\n",
    "plt.colorbar(cbar)\n",
    "plt.xlabel('Omega_c h^2')\n",
    "plt.ylabel('Omega_b h^2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
