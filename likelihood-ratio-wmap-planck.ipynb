{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to build likelihood ratio estimator\n",
    "\n",
    "Estimating \n",
    "\n",
    "$r = \\frac{P(D_W, D_P, \\theta)}{P(D_W, \\theta) P(D_P)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tensionnet import wmapplanck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "                learning_rate=1e-3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(norm_data.shape[1], activation='sigmoid'),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(100, activation='sigmoid',\n",
    "                        kernel_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(100, activation='sigmoid',\n",
    "                        kernel_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(1, activation='linear',\n",
    "                        kernel_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def _test_step(param, truth):\n",
    "        \n",
    "    r\"\"\"\n",
    "    This function is used to calculate the loss value at each epoch and\n",
    "    adjust the weights and biases of the neural networks via the\n",
    "    optimizer algorithm.\n",
    "    \"\"\"\n",
    "    prediction = tf.transpose(model(param, training=True))[0]\n",
    "    prediction = tf.keras.layers.Activation('sigmoid')(prediction)\n",
    "    truth = tf.convert_to_tensor(truth)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(truth, prediction)\n",
    "    return loss\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def _train_step(params, truth):\n",
    "\n",
    "    r\"\"\"\n",
    "    This function is used to calculate the loss value at each epoch and\n",
    "    adjust the weights and biases of the neural networks via the\n",
    "    optimizer algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = tf.transpose(model(params, training=True))[0]\n",
    "        prediction = tf.keras.layers.Activation('sigmoid')(prediction)\n",
    "        truth = tf.convert_to_tensor(truth)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(truth, prediction)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(\n",
    "        zip(gradients,\n",
    "            model.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 50\n",
    "batch_size = 300\n",
    "patience = 10\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = \\\n",
    "        train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "train_dataset = np.hstack([data_train, labels_train[:, np.newaxis]]).astype(np.float32)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "loss_history = []\n",
    "test_loss_history = []\n",
    "c = 0\n",
    "#for i in tqdm.tqdm(range(epochs)):\n",
    "for i in range(epochs):\n",
    "\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "    loss = [_train_step(x[:, :-1], x[:, -1]) for x in  train_dataset]\n",
    "    epoch_loss_avg.update_state(loss)\n",
    "    loss_history.append(epoch_loss_avg.result())\n",
    "\n",
    "    test_loss_history.append(_test_step(data_test, labels_test))\n",
    "    print('Epoch: {} Loss: {:.5f} Test Loss: {:.5f}'.format(\n",
    "        i, loss_history[-1], test_loss_history[-1]))\n",
    "\n",
    "    c += 1\n",
    "    if i == 0:\n",
    "        minimum_loss = test_loss_history[-1]\n",
    "        minimum_epoch = i\n",
    "        minimum_model = model\n",
    "    else:\n",
    "        if test_loss_history[-1] < minimum_loss:\n",
    "            minimum_loss = test_loss_history[-1]\n",
    "            minimum_epoch = i\n",
    "            minimum_model = model\n",
    "            c = 0\n",
    "    if minimum_model:\n",
    "        if c == patience:\n",
    "            print('Early stopped. Epochs used = ' + str(i) +\n",
    "                    '. Minimum at epoch = ' + str(minimum_epoch))\n",
    "            model = minimum_model\n",
    "            break\n",
    "\n",
    "plt.plot(loss_history, label='train')\n",
    "plt.plot(test_loss_history, label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_history.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(tdata, params):\n",
    "\n",
    "        r_values = []\n",
    "        for i in range(len(params)):\n",
    "            ps = tf.convert_to_tensor(np.array([[*tdata, *params[i]]]).astype('float32'))\n",
    "            logr = model(ps).numpy()[0]\n",
    "            r_values.append(logr)\n",
    "\n",
    "        r_values = np.array(r_values).T[0]\n",
    "        return r_values\n",
    "\n",
    "true_params = np.array([0.2, 15, 7])\n",
    "true_data = gaussian(true_params)\n",
    "true_signals = (true_data - signals.mean())/signals.std()\n",
    "\n",
    "\n",
    "samples = prior(10000)\n",
    "norm_params = (samples - theta.mean(axis=0))/theta.std(axis=0)\n",
    "\n",
    "# logL - log Z\n",
    "r_values = __call__(true_signals, norm_params)\n",
    "prior_volume = np.log(1/np.prod(theta_max - theta_min))\n",
    "logP = r_values + prior_volume\n",
    "args = np.argsort(logP)\n",
    "samples = samples[args]\n",
    "logP = logP[args]\n",
    "\n",
    "cbar = plt.scatter(samples[:, 0], samples[:, 1], c=logP- logP.max(), s=10, cmap='viridis')\n",
    "plt.axvline(true_params[0], color='r', linestyle='--')\n",
    "plt.axhline(true_params[1], color='r', linestyle='--')\n",
    "plt.colorbar(cbar)\n",
    "plt.xlabel(r'$A$')\n",
    "plt.ylabel(r'$z_c$')\n",
    "plt.savefig('example_posterior.png', dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
